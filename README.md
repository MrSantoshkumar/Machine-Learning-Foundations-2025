# Foundations-2025

**Foundations in Machine Learning** — a compact portfolio of reproducible, NumPy-first notebooks that walk through key foundations used in AI research and engineering. This repository contains six polished, Kaggle-ready notebooks that showcase a research-oriented mindset: mathematical derivations, from-scratch implementations, diagnostics, and interpretability.

## Contents
- **Linear & Logistic Regression** — closed-form OLS and gradient-descent logistic regression. (NumPy-only)  
- **PCA (from scratch)** — covariance eigendecomposition and SVD implementations; reconstruction and whitening.  
- **Gradient Descent** — batch, mini-batch, and stochastic gradient descent with convergence diagnostics.  
- **Simple Neural Network** — 1-hidden-layer network, forward/backprop, numeric gradient checks, and decision-boundary visualizations.  
- **Probability & Bayes** — conjugacy (Beta-Bernoulli, Normal-Normal), Bayesian linear regression closed form, and illustrative visualizations.  
- **Linear Algebra & Calculus** — eigen/SVD intuition, image low-rank reconstructions, symbolic gradients, Hessians, and numeric checks.

## Quick start (local)
1. Clone the repo:
```bash
git clone https://github.com/MrSantoshKumar/foundations-2025.git
cd foundations-2025
